\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.2}
\rowcolors{2}{blue!5}{white}
\begin{longtable}{|>{\raggedright\arraybackslash}p{3.5cm}|>{\raggedright\arraybackslash}p{4cm}|>{\raggedright\arraybackslash}p{4cm}|>{\raggedright\arraybackslash}p{4cm}|}
\caption{Xy}
\hline
\rowcolor{blue!20}
Environment & A2C & PPO & DDPG \\ 
\hline
\endfirsthead
\hline
\rowcolor{blue!20}
Environment & A2C & PPO & DDPG \\ 
\hline
\endhead
Acrobot & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
n\_steps: 80 \\
policy: MlpPolicy \\
ent\_coef: 0.0
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
n\_steps: 4096 \\
gae\_lambda: 0.94 \\
gamma: 0.99 \\
n\_epochs: 4 \\
ent\_coef: 0.0
\end{tabular} &  \\ 
\hline
CartPole & \scriptsize \begin{tabular}[t]{@{}l@{}}
n\_steps: 40 \\
policy: MlpPolicy \\
ent\_coef: 0.0
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
n\_steps: 256 \\
batch\_size: 256 \\
gae\_lambda: 0.8 \\
gamma: 0.98 \\
n\_epochs: 20 \\
ent\_coef: 0.0 \\
learning\_rate: lin\_0.001 \\
clip\_range: lin\_0.2
\end{tabular} &  \\ 
\hline
MountainCarContinuous & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
n\_steps: 400 \\
policy: MlpPolicy \\
ent\_coef: 0.0 \\
use\_sde: True \\
sde\_sample\_freq: 16 \\
policy\_kwargs: dict(log\_std\_init=0.0, ortho\_init=False)
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 256 \\
n\_steps: 8 \\
gamma: 0.9999 \\
learning\_rate: 7.77e-05 \\
ent\_coef: 0.00429 \\
clip\_range: 0.1 \\
n\_epochs: 10 \\
gae\_lambda: 0.9 \\
max\_grad\_norm: 5 \\
vf\_coef: 0.19 \\
use\_sde: True \\
policy\_kwargs: dict(log\_std\_init=-3.29, ortho\_init=False)
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
noise\_type: ornstein-uhlenbeck \\
noise\_std: 0.5 \\
gradient\_steps: 1 \\
train\_freq: 1 \\
learning\_rate: 0.001 \\
batch\_size: 256 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
MountainCar & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
n\_steps: 80 \\
policy: MlpPolicy \\
ent\_coef: 0.0
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
n\_steps: 256 \\
gae\_lambda: 0.98 \\
gamma: 0.99 \\
n\_epochs: 4 \\
ent\_coef: 0.0
\end{tabular} &  \\ 
\hline
Pendulum & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
ent\_coef: 0.0 \\
max\_grad\_norm: 0.5 \\
n\_steps: 64 \\
gae\_lambda: 0.9 \\
vf\_coef: 0.4 \\
gamma: 0.9 \\
use\_rms\_prop: True \\
normalize\_advantage: False \\
learning\_rate: lin\_7e-4 \\
use\_sde: True \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False)
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
n\_steps: 4096 \\
gae\_lambda: 0.95 \\
gamma: 0.9 \\
n\_epochs: 10 \\
ent\_coef: 0.0 \\
learning\_rate: 0.001 \\
clip\_range: 0.2 \\
use\_sde: True \\
sde\_sample\_freq: 4
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
gamma: 0.98 \\
buffer\_size: 200000 \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
gradient\_steps: 1 \\
train\_freq: 1 \\
learning\_rate: 0.001 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
BipedalWalker & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
ent\_coef: 0.0 \\
max\_grad\_norm: 0.5 \\
n\_steps: 128 \\
gae\_lambda: 0.9 \\
vf\_coef: 0.4 \\
gamma: 0.99 \\
use\_rms\_prop: True \\
normalize\_advantage: False \\
learning\_rate: lin\_0.00096 \\
use\_sde: True \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False)
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
n\_steps: 65536 \\
batch\_size: 64 \\
gae\_lambda: 0.95 \\
gamma: 0.999 \\
n\_epochs: 10 \\
ent\_coef: 0.0 \\
learning\_rate: 0.0003 \\
clip\_range: 0.18
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
gamma: 0.98 \\
buffer\_size: 200000 \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
gradient\_steps: 1 \\
train\_freq: 1 \\
learning\_rate: 0.001 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
CarRacing & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: {'norm\_obs': False, 'norm\_reward': True} \\
policy: CnnPolicy \\
n\_steps: 512 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
ent\_coef: 0.0 \\
sde\_sample\_freq: 4 \\
max\_grad\_norm: 0.5 \\
vf\_coef: 0.5 \\
learning\_rate: lin\_1e-4 \\
use\_sde: True \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False, activation\_fn=nn.GELU, net\_arch=dict(pi=[256], vf=[256]))
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: {'norm\_obs': False, 'norm\_reward': True} \\
policy: CnnPolicy \\
batch\_size: 128 \\
n\_steps: 4096 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
n\_epochs: 10 \\
ent\_coef: 0.0 \\
sde\_sample\_freq: 4 \\
max\_grad\_norm: 0.5 \\
vf\_coef: 0.5 \\
learning\_rate: lin\_1e-4 \\
use\_sde: True \\
clip\_range: 0.2 \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False, activation\_fn=nn.GELU, net\_arch=dict(pi=[256], vf=[256]))
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: CnnPolicy \\
buffer\_size: 1000000
\end{tabular} \\ 
\hline
LunarLander & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
ent\_coef: 0.0 \\
max\_grad\_norm: 0.5 \\
n\_steps: 32 \\
gae\_lambda: 0.9 \\
vf\_coef: 0.4 \\
gamma: 0.99 \\
use\_rms\_prop: True \\
normalize\_advantage: False \\
learning\_rate: lin\_7e-4 \\
use\_sde: True \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False)
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
n\_steps: 16384 \\
batch\_size: 64 \\
gae\_lambda: 0.98 \\
gamma: 0.999 \\
n\_epochs: 4 \\
ent\_coef: 0.01
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
gamma: 0.98 \\
buffer\_size: 200000 \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
gradient\_steps: 1 \\
train\_freq: 1 \\
learning\_rate: 0.001 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
Blackjack &  &  &  \\ 
\hline
Taxi &  &  &  \\ 
\hline
CliffWalking &  &  &  \\ 
\hline
FrozenLake &  &  &  \\ 
\hline
Ant & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0005 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 1.0 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 1024 \\
n\_steps: 16384
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
train\_freq: 1 \\
gradient\_steps: 1 \\
learning\_rate: 0.001 \\
batch\_size: 256 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
HalfCheetah & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0007 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 0.5 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 512 \\
n\_steps: 8192 \\
gamma: 0.98 \\
learning\_rate: 2.0633e-05 \\
ent\_coef: 0.000401762 \\
clip\_range: 0.1 \\
n\_epochs: 20 \\
gae\_lambda: 0.92 \\
max\_grad\_norm: 0.8 \\
vf\_coef: 0.58096 \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False, activation\_fn=nn.ReLU, net\_arch=dict(pi=[256, 256], vf=[256, 256]))
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
train\_freq: 1 \\
gradient\_steps: 1 \\
learning\_rate: 0.001 \\
batch\_size: 256 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
Hopper & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0007 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 0.5 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 512 \\
n\_steps: 8192 \\
gamma: 0.999 \\
learning\_rate: 9.80828e-05 \\
ent\_coef: 0.00229519 \\
clip\_range: 0.2 \\
n\_epochs: 5 \\
gae\_lambda: 0.99 \\
max\_grad\_norm: 0.7 \\
vf\_coef: 0.835671 \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False, activation\_fn=nn.ReLU, net\_arch=dict(pi=[256, 256], vf=[256, 256]))
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
train\_freq: 1 \\
gradient\_steps: 1 \\
learning\_rate: 0.001 \\
batch\_size: 256 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
Humanoid & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0005 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 1.0 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 1024 \\
n\_steps: 16384 \\
gamma: 0.95 \\
learning\_rate: 3.56987e-05 \\
ent\_coef: 0.00238306 \\
clip\_range: 0.3 \\
n\_epochs: 5 \\
gae\_lambda: 0.9 \\
max\_grad\_norm: 2 \\
vf\_coef: 0.431892 \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False, activation\_fn=nn.ReLU, net\_arch=dict(pi=[256, 256], vf=[256, 256]))
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
train\_freq: 1 \\
gradient\_steps: 1 \\
learning\_rate: 0.001 \\
batch\_size: 256 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
HumanoidStandup & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0005 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 1.0 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 1024 \\
n\_steps: 16384 \\
gamma: 0.99 \\
learning\_rate: 2.55673e-05 \\
ent\_coef: 3.62109e-06 \\
clip\_range: 0.3 \\
n\_epochs: 20 \\
gae\_lambda: 0.9 \\
max\_grad\_norm: 0.7 \\
vf\_coef: 0.430793 \\
policy\_kwargs: dict(log\_std\_init=-2, ortho\_init=False, activation\_fn=nn.ReLU, net\_arch=dict(pi=[256, 256], vf=[256, 256]))
\end{tabular} &  \\ 
\hline
InvertedDoublePendulum & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 128 \\
n\_steps: 512 \\
gamma: 0.98 \\
learning\_rate: 0.000155454 \\
ent\_coef: 1.05057e-06 \\
clip\_range: 0.4 \\
n\_epochs: 10 \\
gae\_lambda: 0.8 \\
max\_grad\_norm: 0.5 \\
vf\_coef: 0.695929
\end{tabular} &  \\ 
\hline
InvertedPendulum & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 32 \\
n\_steps: 64 \\
gamma: 0.999 \\
learning\_rate: 0.000222425 \\
ent\_coef: 1.37976e-07 \\
clip\_range: 0.4 \\
n\_epochs: 5 \\
gae\_lambda: 0.9 \\
max\_grad\_norm: 0.3 \\
vf\_coef: 0.19816
\end{tabular} &  \\ 
\hline
Pusher & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0007 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 0.5 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 512 \\
n\_steps: 8192
\end{tabular} &  \\ 
\hline
Reacher & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0007 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 0.5 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 512 \\
n\_steps: 8192 \\
gamma: 0.9 \\
learning\_rate: 0.000104019 \\
ent\_coef: 7.52585e-08 \\
clip\_range: 0.3 \\
n\_epochs: 5 \\
gae\_lambda: 1.0 \\
max\_grad\_norm: 0.9 \\
vf\_coef: 0.950368
\end{tabular} &  \\ 
\hline
Swimmer & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0007 \\
gamma: 0.9999 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 0.5 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
gamma: 0.9999 \\
batch\_size: 128 \\
n\_steps: 2048 \\
learning\_rate: 0.0006 \\
gae\_lambda: 0.98
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
train\_freq: 1 \\
gradient\_steps: 1 \\
learning\_rate: 0.001 \\
batch\_size: 256 \\
policy\_kwargs: dict(net\_arch=[400, 300]) \\
gamma: 0.9999
\end{tabular} \\ 
\hline
Walker2d & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
learning\_rate: 0.0007 \\
gamma: 0.99 \\
gae\_lambda: 0.95 \\
vf\_coef: 0.5 \\
ent\_coef: 0.01 \\
max\_grad\_norm: 0.5 \\
use\_rms\_prop: True \\
n\_steps: 512
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
normalize: True \\
policy: MlpPolicy \\
batch\_size: 512 \\
n\_steps: 8192 \\
gamma: 0.99 \\
learning\_rate: 5.05041e-05 \\
ent\_coef: 0.000585045 \\
clip\_range: 0.1 \\
n\_epochs: 20 \\
gae\_lambda: 0.95 \\
max\_grad\_norm: 1 \\
vf\_coef: 0.871923
\end{tabular} & \scriptsize \begin{tabular}[t]{@{}l@{}}
policy: MlpPolicy \\
learning\_starts: 10000 \\
noise\_type: normal \\
noise\_std: 0.1 \\
train\_freq: 1 \\
gradient\_steps: 1 \\
learning\_rate: 0.001 \\
batch\_size: 256 \\
policy\_kwargs: dict(net\_arch=[400, 300])
\end{tabular} \\ 
\hline
\end{longtable}
\label{tab:hyperparams_table}
